## 1. Introduction to Prompts

Prompts are the second core component of the LangChain framework. They are defined as **input instructions or queries** given to a model to guide and shape its output. In essence, every message you send to a Large Language Model (LLM) is a prompt.

### Types of Prompts

- **Text-based Prompts:** Textual instructions and interactions (the primary focus of current applications).
- **Multimodal Prompts:** Interactions involving text combined with other modes like **images, sound, or video**.

---

## 2. Static vs. Dynamic Prompts

### Static Prompts

These are prompts where the programmer hardcodes the instruction directly into the `invoke()` function.

- **The Problem:** In real-world apps, you cannot hardcode prompts. Allowing users to type the entire prompt (static input) gives them too much control, which can lead to **hallucinations** or inconsistent user experiences if they provide vague or incorrect instructions.

### Dynamic Prompts

These use **templates** with specific "fill-in-the-blank" placeholders.

- **The Solution:** The developer creates a fixed structure (e.g., for a research summariser) and the user only provides specific variables like the Paper Title, Style, and Length. This ensures the output always follows a professional, high-quality format defined by the developer.

---

## 3. The `PromptTemplate` Class

LangChain provides the `PromptTemplate` class to manage dynamic prompts.

### Why use `PromptTemplate` over standard Python f-strings?

While f-strings can work, `PromptTemplate` offers three major advantages:

1.  **Validation:** It can automatically verify if all required placeholders are present before the code runs, preventing runtime errors on the server.
2.  **Reusability:** You can save templates as **JSON files**, allowing them to be loaded and shared across different parts of a large application or different web pages.
3.  **Tight Coupling:** It is designed to fit perfectly into the **LangChain ecosystem**, specifically for creating **Chains** where multiple steps (template + model) are tied together in a single execution.

---

## 4. Building Chatbots and Managing History

A basic chatbot needs to remember the context of the conversation. Without **Chat History**, the model cannot answer follow-up questions (e.g., "Multiply that number by 10") because it doesn't remember the "number" from the previous message.

### Message Roles in LangChain

To help the LLM understand who said what, LangChain uses three specific message classes:

- **SystemMessage:** Sets the "persona" or rules for the AI (e.g., "You are a helpful doctor").
- **HumanMessage:** Represents the input coming from the user.
- **AIMessage:** Represents the response generated by the AI.

By sending a **list of these labelled messages** as chat history, the model maintains full context and understands the flow of conversation.

---

## 5. Advanced Prompting Classes

### ChatPromptTemplate

While `PromptTemplate` is for single-turn queries, `ChatPromptTemplate` is used for **multi-turn (conversational) templates**. It allows you to define dynamic placeholders within different roles (System, Human, AI).

- **Best Practice:** In the latest LangChain versions, it is recommended to define these using **tuples** (e.g., `("system", "Your instruction")`) for better compatibility.

### MessagesPlaceholder

This is a special placeholder used inside a `ChatPromptTemplate` to **dynamically insert a list of messages** (like chat history) at runtime.

- **Use Case:** If a user returns to a chatbot after two days, you can fetch their previous conversation from a database and "plug" it into the `MessagesPlaceholder` so the AI instantly regains the context of the old conversation.

---
